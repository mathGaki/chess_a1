import os
import glob
import time
import torch
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F
import csv
from datetime import datetime
try:
    # PyTorch 2.0+ ìƒˆë¡œìš´ API
    from torch.amp import autocast as torch_autocast, GradScaler as torch_GradScaler
    def get_autocast(device_type='cuda'):
        return torch_autocast(device_type=device_type, dtype=torch.float16)
    def get_gradscaler(device='cuda'):
        return torch_GradScaler(device)
except ImportError:
    # PyTorch 1.x ì´ì „ API
    from torch.cuda.amp import autocast as torch_autocast, GradScaler as torch_GradScaler
    def get_autocast(device_type='cuda'):
        return torch_autocast()
    def get_gradscaler(device='cuda'):
        return torch_GradScaler()

from .neural_network import ChessNet, INPUT_CHANNELS, ACTION_SPACE_SIZE
from .config import learning_rate, weight_decay, batch_size, training_steps, LR_SCHEDULE

class SelfPlayDataset(Dataset):
    """ìê¸° ëŒ€êµ­ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê¸° ìœ„í•œ PyTorch Dataset (ìµœì í™”ëœ ë²„ì „)"""
    def __init__(self, data_dir='data/selfplay'):
        self.data_files = glob.glob(os.path.join(data_dir, '*.pt'))
        self.samples = []
        for file_path in self.data_files:
            # ê° íŒŒì¼ì€ (state, policy, value) íŠœí”Œì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ í¬í•¨
            data = torch.load(file_path, map_location='cpu')  # CPUì— ë¨¼ì € ë¡œë“œ
            self.samples.extend(data)
        print(f"Loaded {len(self.data_files)} game files with {len(self.samples)} total positions.")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        state, policy_target, value_target = self.samples[idx]
        
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ float32ë¡œ ë³€í™˜ (mixed precisionê³¼ í˜¸í™˜)
        if state.dtype != torch.float32:
            state = state.float()
        if policy_target.dtype != torch.float32:
            policy_target = policy_target.float()
        if value_target.dtype != torch.float32:
            value_target = value_target.float()
        
        # ë°ì´í„° ê²€ì¦ ë° í´ë¦¬í•‘
        state = torch.clamp(state, -10, 10)
        value_target = torch.clamp(value_target, -1, 1)
        
        # Policy target ì •ê·œí™”
        policy_target = policy_target / (policy_target.sum() + 1e-8)
        policy_target = torch.clamp(policy_target, 1e-8, 1.0)
        
        # NaN ì²´í¬
        if torch.isnan(state).any() or torch.isnan(policy_target).any() or torch.isnan(value_target).any():
            print(f"âš ï¸ NaN in data at index {idx}! Using fallback data.")
            # ì•ˆì „í•œ ëŒ€ì²´ ë°ì´í„° ìƒì„±
            state = torch.zeros_like(state)
            policy_target = torch.ones_like(policy_target) / len(policy_target)
            value_target = torch.tensor([0.0])
            
        return state, policy_target, value_target

class Trainer:
    """ì‹ ê²½ë§ í•™ìŠµì„ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤"""
    def __init__(self, model, device, config):
        self.model = model.to(device)
        self.device = device
        self.config = config
        
        # ëª¨ë¸ ê°€ì¤‘ì¹˜ ê²€ì¦ ë° ì´ˆê¸°í™”
        print("ğŸ”„ Checking model weights...")
        self._verify_model_weights()
        
        # PyTorch 2.0+ ì»´íŒŒì¼ ìµœì í™” (RTX 5070ì—ì„œ 20-30% ì†ë„ í–¥ìƒ)
        if (hasattr(torch, 'compile') and 
            device.type == 'cuda' and 
            config.get('ENABLE_TORCH_COMPILE', True)):
            try:
                # Triton í˜¸í™˜ì„± ì²´í¬ ë° ì•ˆì „í•œ ì»´íŒŒì¼
                test_tensor = torch.randn(1, 1, device=device)
                torch.compile(lambda x: x + 1)(test_tensor)  # í…ŒìŠ¤íŠ¸ ì»´íŒŒì¼
                
                compile_mode = config.get('TORCH_COMPILE_MODE', 'reduce-overhead')
                self.model = torch.compile(self.model, mode=compile_mode)
                print(f"ğŸš€ Model compiled with PyTorch 2.0+ ({compile_mode}) for faster training")
            except Exception as e:
                print(f"âš ï¸ Model compilation failed (using standard training): {e}")
                print("ğŸ’¡ Tip: Check if triton-windows is properly installed")
                print("ğŸ’¡ Or set ENABLE_TORCH_COMPILE=False in config.py to disable compilation")
        else:
            print("ğŸ“ Using standard PyTorch training (compilation disabled)")
        
        self.optimizer = optim.Adam(self.model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])
        
        # AlphaZero í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ (RTX 5070ì—ì„œ ë” ì•ˆì •ì ì¸ í•™ìŠµ)
        from torch.optim.lr_scheduler import PolynomialLR
        self.scheduler = PolynomialLR(self.optimizer, total_iters=config.get('training_steps', 700), power=0.9)
        
        # Mixed Precision Trainingì„ ìœ„í•œ GradScaler (GPUì—ì„œë§Œ ì‚¬ìš©)
        self.use_amp = (device.type == 'cuda' and 
                       config.get('ENABLE_MIXED_PRECISION', True))
        if self.use_amp:
            self.scaler = get_gradscaler('cuda')
            print("ğŸš€ Using Mixed Precision Training (AMP) for faster training")
        else:
            print("âš ï¸ AMP disabled (CPU mode or disabled in config)")
            self.scaler = None

    def _verify_model_weights(self):
        """ëª¨ë¸ ê°€ì¤‘ì¹˜ê°€ ìœ íš¨í•œì§€ ê²€ì¦í•˜ê³  ë¬¸ì œê°€ ìˆìœ¼ë©´ ì¬ì´ˆê¸°í™”"""
        has_nan = False
        has_inf = False
        
        for name, param in self.model.named_parameters():
            if torch.isnan(param).any():
                print(f"âš ï¸ NaN detected in {name}!")
                has_nan = True
            if torch.isinf(param).any():
                print(f"âš ï¸ Inf detected in {name}!")
                has_inf = True
        
        if has_nan or has_inf:
            print("ğŸ”„ Reinitializing model weights...")
            # ëª¨ë¸ ì¬ì´ˆê¸°í™”
            for module in self.model.modules():
                if hasattr(module, 'reset_parameters'):
                    module.reset_parameters()
                elif isinstance(module, torch.nn.Linear):
                    torch.nn.init.xavier_uniform_(module.weight)
                    if module.bias is not None:
                        torch.nn.init.zeros_(module.bias)
                elif isinstance(module, torch.nn.Conv2d):
                    torch.nn.init.xavier_uniform_(module.weight)
                    if module.bias is not None:
                        torch.nn.init.zeros_(module.bias)
            print("âœ… Model weights reinitialized")
        else:
            print("âœ… Model weights are valid")

    def train(self, dataset):
        """AlphaZero ë°©ì‹: ì—í¬í¬ ì—†ì´ ê³ ì •ëœ ìˆ˜ì˜ training steps ì‹¤í–‰"""
        print("ğŸ”„ Setting model to training mode...")
        self.model.train()
        
        # ë…¼ë¬¸ìš© í•™ìŠµ í†µê³„ ë¡œê¹… ì„¤ì •
        os.makedirs('logs', exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        log_file = f'logs/training_log_{timestamp}.csv'
        
        # CSV íŒŒì¼ í—¤ë” ì‘ì„±
        with open(log_file, 'w', newline='') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(['step', 'total_step', 'policy_loss', 'value_loss', 'total_loss', 'learning_rate', 'elapsed_time'])
        
        print(f"ğŸ“Š Training statistics will be logged to: {log_file}")
        
        # ìµœì í™”ëœ DataLoader ì„¤ì • (Windows í˜¸í™˜ì„± ìš°ì„ )
        num_workers = 0  # 2 â†’ 0 (Windowsì—ì„œ multiprocessing ë°ë“œë½ ë°©ì§€)
        print(f"ğŸ”„ Creating DataLoader with {num_workers} workers...")
        dataloader = DataLoader(
            dataset, 
            batch_size=self.config['batch_size'], 
            shuffle=True, 
            num_workers=num_workers,
            pin_memory=True,  # GPU ì „ì†¡ ì†ë„ í–¥ìƒ
            persistent_workers=False,  # Windows í˜¸í™˜ì„±ì„ ìœ„í•´ ë¹„í™œì„±í™”
            prefetch_factor=None if num_workers == 0 else 4,  # num_workers=0ì¼ ë•Œ None í•„ìš”
            drop_last=True  # ë§ˆì§€ë§‰ ë°°ì¹˜ í¬ê¸° ë¶ˆì¼ì¹˜ ë°©ì§€
        )
        
        print("ğŸ”„ Creating infinite dataloader...")
        # ë°ì´í„°ì…‹ì„ ë¬´í•œíˆ ë°˜ë³µí•˜ê¸° ìœ„í•œ ì´í„°ë ˆì´í„°
        from itertools import cycle
        infinite_dataloader = cycle(dataloader)
        
        total_loss = 0
        print(f"Starting training for {self.config['training_steps']} steps...")
        print(f"Current total steps: {self.config.get('total_steps', 0)}")
        print(f"Initial learning rate: {self._get_learning_rate(self.config.get('total_steps', 0)):.6f}")
        print(f"Batch size: {self.config['batch_size']}, Workers: {num_workers}")
        
        # RTX 5070 ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”
        if self.device.type == 'cuda':
            print("ğŸ”„ Setting up GPU optimizations...")
            torch.cuda.empty_cache()  # ì‹œì‘ ì „ GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            torch.backends.cudnn.benchmark = True  # cuDNN ìë™ ìµœì í™”
            print(f"ğŸš€ GPU Memory: {torch.cuda.get_device_properties(0).total_memory // 1024**3}GB available")
            
        # ì„±ëŠ¥ ì¸¡ì • ì‹œì‘
        training_start_time = time.time()
        step_start_time = time.time()
        
        print("ğŸ”„ Starting training loop...")
        for step in range(self.config['training_steps']):
            try:
                # AlphaZero ìŠ¤íƒ€ì¼ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ (ì „ì²´ ëˆ„ì  ìŠ¤í… ê¸°ì¤€)
                total_step = self.config.get('total_steps', 0) + step
                current_lr = self._get_learning_rate(total_step)
                for param_group in self.optimizer.param_groups:
                    param_group['lr'] = current_lr
                
                # ë‹¤ìŒ ë°°ì¹˜ ê°€ì ¸ì˜¤ê¸°
                states, policy_targets, value_targets = next(infinite_dataloader)
                states = states.to(self.device, non_blocking=True)  # ë¹„ë™ê¸° ì „ì†¡
                policy_targets = policy_targets.to(self.device, non_blocking=True)
                value_targets = value_targets.to(self.device, non_blocking=True)

                # ì…ë ¥ ë°ì´í„° ê²€ì¦ ë° ì •ê·œí™”
                if torch.isnan(states).any() or torch.isnan(policy_targets).any() or torch.isnan(value_targets).any():
                    print(f"âš ï¸ NaN in input data! Skipping batch...")
                    continue
                
                # ì…ë ¥ ë°ì´í„° í´ë¦¬í•‘ (ê·¹ê°’ ë°©ì§€)
                states = torch.clamp(states, -10, 10)
                value_targets = torch.clamp(value_targets, -1, 1)
                
                # Policy targets ì •ê·œí™” (í™•ë¥  ë¶„í¬ ë³´ì¥)
                policy_targets = policy_targets / (policy_targets.sum(dim=1, keepdim=True) + 1e-8)

                self.optimizer.zero_grad()

                # Mixed Precision Forward Pass
                if self.use_amp:
                    with get_autocast('cuda'):
                        policy_log_preds, value_preds = self.model(states)
                        
                        # Value loss ê³„ì‚° (í´ë¦¬í•‘ ì¶”ê°€)
                        value_preds = torch.clamp(value_preds, -1, 1)
                        value_loss = F.mse_loss(value_preds.squeeze(), value_targets.squeeze())
                        
                        # Policy loss ê³„ì‚° (ì•ˆì „í•œ KL divergence)
                        # log_softmax ê²°ê³¼ë¥¼ softmaxë¡œ ë³€í™˜í•˜ì—¬ í™•ë¥  ë¶„í¬ë¡œ ë§Œë“¤ê¸°
                        policy_probs = F.softmax(policy_log_preds, dim=1)
                        policy_probs = torch.clamp(policy_probs, 1e-8, 1.0)  # 0 ë°©ì§€
                        policy_targets = torch.clamp(policy_targets, 1e-8, 1.0)  # 0 ë°©ì§€
                        
                        # KL divergence ê³„ì‚° (ë” ì•ˆì „í•œ ë°©ì‹)
                        policy_loss = F.kl_div(torch.log(policy_probs), policy_targets, reduction='batchmean', log_target=False)
                        
                        # NaN ì²´í¬ ë° ì²˜ë¦¬
                        if torch.isnan(value_loss) or torch.isnan(policy_loss) or torch.isinf(value_loss) or torch.isinf(policy_loss):
                            print(f"âš ï¸ NaN/Inf detected! value_loss: {value_loss}, policy_loss: {policy_loss}")
                            print(f"   value_preds range: [{value_preds.min():.6f}, {value_preds.max():.6f}]")
                            print(f"   policy_probs range: [{policy_probs.min():.6f}, {policy_probs.max():.6f}]")
                            continue
                        
                        loss = value_loss + policy_loss
                    
                    # Mixed Precision Backward Pass
                    self.scaler.scale(loss).backward()
                    
                    # AlphaZero ì›ë³¸ ë°©ì‹: L2 ì •ê·œí™”ë§Œ ì‚¬ìš©, ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ ì œê±°
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                    
                    # ë…¼ë¬¸ìš© í†µê³„ ë¡œê¹… (ê° loss ê°œë³„ ê¸°ë¡)
                    step_log_data = {
                        'step': step + 1,
                        'total_step': self.config.get('total_steps', 0) + step + 1,
                        'policy_loss': policy_loss.item(),
                        'value_loss': value_loss.item(),
                        'total_loss': loss.item(),
                        'learning_rate': current_lr,
                        'elapsed_time': time.time() - step_start_time if 'step_start_time' in locals() else 0
                    }
                    
                    # CSV íŒŒì¼ì— ê¸°ë¡
                    with open(log_file, 'a', newline='') as csvfile:
                        writer = csv.writer(csvfile)
                        writer.writerow([
                            step_log_data['step'],
                            step_log_data['total_step'],
                            step_log_data['policy_loss'],
                            step_log_data['value_loss'],
                            step_log_data['total_loss'],
                            step_log_data['learning_rate'],
                            step_log_data['elapsed_time']
                        ])
                else:
                    # ì¼ë°˜ Forward/Backward Pass (CPU ëª¨ë“œ)
                    policy_log_preds, value_preds = self.model(states)
                    value_loss = F.mse_loss(value_preds.squeeze(), value_targets.squeeze())
                    
                    # KL divergence ì•ˆì „ ê³„ì‚° (NaN ë°©ì§€)
                    policy_loss = F.kl_div(policy_log_preds, policy_targets, reduction='batchmean', log_target=False)
                    
                    # NaN ì²´í¬ ë° ì²˜ë¦¬
                    if torch.isnan(value_loss) or torch.isnan(policy_loss):
                        print(f"âš ï¸ NaN detected! value_loss: {value_loss}, policy_loss: {policy_loss}")
                        continue
                    
                    loss = value_loss + policy_loss
                    
                    loss.backward()
                    
                    # AlphaZero ì›ë³¸ ë°©ì‹: L2 ì •ê·œí™”ë§Œ ì‚¬ìš©, ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ ì œê±°
                    self.optimizer.step()
                    
                    # ë…¼ë¬¸ìš© í†µê³„ ë¡œê¹… (ê° loss ê°œë³„ ê¸°ë¡)
                    step_log_data = {
                        'step': step + 1,
                        'total_step': self.config.get('total_steps', 0) + step + 1,
                        'policy_loss': policy_loss.item(),
                        'value_loss': value_loss.item(),
                        'total_loss': loss.item(),
                        'learning_rate': current_lr,
                        'elapsed_time': time.time() - step_start_time if 'step_start_time' in locals() else 0
                    }
                    
                    # CSV íŒŒì¼ì— ê¸°ë¡
                    with open(log_file, 'a', newline='') as csvfile:
                        writer = csv.writer(csvfile)
                        writer.writerow([
                            step_log_data['step'],
                            step_log_data['total_step'],
                            step_log_data['policy_loss'],
                            step_log_data['value_loss'],
                            step_log_data['total_loss'],
                            step_log_data['learning_rate'],
                            step_log_data['elapsed_time']
                        ])
                
                total_loss += loss.item()
                
            except Exception as e:
                print(f"âŒ Error at step {step + 1}: {e}")
                import traceback
                traceback.print_exc()
                break
            
            # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
            self.scheduler.step()
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥ (ë§¤ 100 ìŠ¤í…ë§ˆë‹¤)
            if (step + 1) % 100 == 0:
                avg_loss = total_loss / 100
                total_step = self.config.get('total_steps', 0) + step
                elapsed_time = time.time() - step_start_time if 'step_start_time' in locals() else 0
                
                print(f"Step {step + 1}/{self.config['training_steps']} (Total: {total_step + 1}): "
                      f"Loss = {avg_loss:.6f}, LR = {current_lr:.6f}, "
                      f"Time = {elapsed_time:.2f}s")
                
                total_loss = 0
                step_start_time = time.time()
                
                # RTX 5070 ë©”ëª¨ë¦¬ ê´€ë¦¬ (ë§¤ 100 ìŠ¤í…ë§ˆë‹¤)
                if self.device.type == 'cuda' and (step + 1) % 100 == 0:
                    torch.cuda.empty_cache()
        
        # í•™ìŠµ ì™„ë£Œ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬
        if self.device.type == 'cuda':
            torch.cuda.empty_cache()
            
        # ìµœì¢… ì„±ëŠ¥ ë¦¬í¬íŠ¸ (RTX 5070 ë²¤ì¹˜ë§ˆí‚¹)
        total_training_time = time.time() - training_start_time
        steps_per_second = self.config['training_steps'] / total_training_time
        time_per_100_steps = (total_training_time / self.config['training_steps']) * 100
        
        final_avg_loss = total_loss / (step + 1) if step > 0 else 0
        print(f"\nâœ… Training completed! Final average loss: {final_avg_loss:.6f}")
        print(f"ğŸš€ RTX 5070 Performance Report:")
        print(f"   ğŸ“Š Total training time: {total_training_time:.2f} seconds")
        print(f"   âš¡ Steps per second: {steps_per_second:.2f}")
        print(f"   â±ï¸ Time per 100 steps: {time_per_100_steps:.2f} seconds")
        print(f"   ğŸ¯ Target time achieved: {time_per_100_steps < 300:.0f} (< 5 minutes)")
        
        return final_avg_loss
    
    def _get_learning_rate(self, step):
        """AlphaZero ë°©ì‹ì˜ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§"""
        for threshold in sorted(LR_SCHEDULE.keys(), reverse=True):
            if step >= threshold:
                return LR_SCHEDULE[threshold]
        return LR_SCHEDULE[0]
            
    def save_model(self, path):
        print(f"Saving model to {path}")
        torch.save(self.model.state_dict(), path)

# --- ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„ (í…ŒìŠ¤íŠ¸ìš©) ---
if __name__ == '__main__':
    # ê°€ìƒ ë°ì´í„° ìƒì„±
    if not os.path.exists('data/selfplay'):
        os.makedirs('data/selfplay')
    
    dummy_data = []
    for _ in range(100):
        state = torch.randn(INPUT_CHANNELS, 8, 8)
        policy = torch.randn(ACTION_SPACE_SIZE)
        policy = F.softmax(policy, dim=0)
        value = torch.tensor([1.0])
        dummy_data.append((state, policy, value))
    torch.save(dummy_data, 'data/selfplay/dummy_game.pt')

    # í•™ìŠµ ì„¤ì • (config.pyì—ì„œ ê°€ì ¸ì˜¤ê¸° - AlphaZero ë°©ì‹)
    train_config = {
        'learning_rate': learning_rate,
        'weight_decay': weight_decay,
        'batch_size': batch_size,
        'training_steps': training_steps  # epochs ëŒ€ì‹  training_steps ì‚¬ìš©
    }
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = ChessNet()
    
    dataset = SelfPlayDataset()
    trainer = Trainer(model, device, train_config)
    trainer.train(dataset)
    
    if not os.path.exists('models'):
        os.makedirs('models')
    trainer.save_model('models/trained_model.pth')
