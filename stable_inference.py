# src/ai/stable_inference.py
"""
ì•ˆì •ì ì¸ ì¶”ë¡  ì„œë²„ (pickle ë¬¸ì œ í•´ê²°)
"""
import time
import torch
import torch.multiprocessing as mp
from .neural_network import ChessNet
from .config import INFERENCE_TIMEOUT, INFERENCE_BATCH_SIZE
import queue

def stable_inference_server(model_path, device, request_queue, response_queue, stop_event):
    """ì•ˆì •ì ì¸ ì¶”ë¡  ì„œë²„"""
    print(f"ğŸš€ Stable inference server starting on {device}")
    
    # ëª¨ë¸ ë¡œë“œ
    model = ChessNet()
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.to(device)
    model.eval()
    
    # TorchScript ìµœì í™”
    try:
        model = torch.jit.script(model)
        print("ğŸ”¥ Model optimized with TorchScript")
    except Exception as e:
        print(f"âš ï¸ TorchScript failed: {e}")
    
    print(f"ğŸš€ Stable inference server started on {device}")
    
    batch_count = 0
    total_requests = 0
    batch_requests = []
    last_process_time = time.time()
    start_time = time.time()  # ì „ì²´ ì‹œì‘ ì‹œê°„ ì¶”ê°€
    
    while not stop_event.is_set():
        try:
            # ë°°ì¹˜ ìˆ˜ì§‘ - config ê¸°ë°˜ ë‹¨ìˆœí™”
            max_collect_time = INFERENCE_TIMEOUT / 10
            collect_start = time.time()
            
            while len(batch_requests) < 512 and (time.time() - collect_start) < max_collect_time:
                try:
                    req = request_queue.get_nowait()
                    batch_requests.append(req)
                except queue.Empty:
                    break
            
            # ì‹¤ì œ í…ì„œ ê°œìˆ˜ ê³„ì‚°
            total_tensors = 0
            for _, _, state in batch_requests:
                if isinstance(state, list):
                    total_tensors += len(state)
                else:
                    total_tensors += 1
            
            # ë‹¨ìˆœí™”ëœ ë°°ì¹˜ ì²˜ë¦¬ ì¡°ê±´
            current_time = time.time()
            time_since_last = current_time - last_process_time
            
            # ë°°ì¹˜ ì²˜ë¦¬ ì¡°ê±´ (ë‹¨ìˆœí™”):
            # 1. í…ì„œ ìˆ˜ê°€ config ë°°ì¹˜ í¬ê¸°ì— ë„ë‹¬í•˜ë©´ ì¦‰ì‹œ ì²˜ë¦¬
            # 2. ë˜ëŠ” íƒ€ì„ì•„ì›ƒì— ë„ë‹¬í•˜ë©´ ì²˜ë¦¬ (ìµœì†Œ 1ê°œ ìš”ì²­ ìˆì„ ë•Œ)
            should_process = (
                total_tensors >= INFERENCE_BATCH_SIZE or  # config ë°°ì¹˜ í¬ê¸° ë„ë‹¬
                (len(batch_requests) > 0 and time_since_last > INFERENCE_TIMEOUT)  # íƒ€ì„ì•„ì›ƒ + ìµœì†Œ ìš”ì²­
            )
            
            if should_process:
                if batch_requests:
                    # ë°°ì¹˜ ì²˜ë¦¬ - ê°œë³„ ìš”ì²­ê³¼ ë°°ì¹˜ ìš”ì²­ êµ¬ë¶„
                    worker_ids, request_ids, states = zip(*batch_requests)
                    
                    # ë°°ì¹˜ ìš”ì²­ê³¼ ê°œë³„ ìš”ì²­ ë¶„ë¦¬
                    individual_requests = []
                    batch_requests_list = []
                    
                    for worker_id, request_id, state in zip(worker_ids, request_ids, states):
                        if isinstance(state, list):
                            # ìƒˆë¡œìš´ ë°°ì¹˜ ì‹œìŠ¤í…œ: stateëŠ” í…ì„œ ë¦¬ìŠ¤íŠ¸
                            batch_requests_list.append((worker_id, request_id, state))
                        else:
                            # ê°œë³„ ìš”ì²­: stateëŠ” ë‹¨ì¼ í…ì„œ
                            individual_requests.append((worker_id, request_id, state))
                    
                    # ê°œë³„ ìš”ì²­ ì²˜ë¦¬
                    if individual_requests:
                        ind_worker_ids, ind_request_ids, ind_states = zip(*individual_requests)
                        ind_state_batch = torch.stack(ind_states).to(device)
                        
                        with torch.no_grad():
                            ind_policy_logits, ind_values = model(ind_state_batch)
                        
                        # ê°œë³„ ì‘ë‹µ ì „ì†¡
                        for i, (worker_id, request_id, _) in enumerate(individual_requests):
                            response_queue.put((worker_id, request_id, ind_policy_logits[i].cpu(), ind_values[i].cpu()))
                    
                    # ë°°ì¹˜ ìš”ì²­ ì²˜ë¦¬ (ìƒˆë¡œìš´ ì‹œìŠ¤í…œ)
                    for worker_id, request_id, batch_tensors in batch_requests_list:
                        if len(batch_tensors) > 0:
                            # í…ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ë³€í™˜
                            batch_state = torch.stack(batch_tensors).to(device)
                            
                            with torch.no_grad():
                                batch_policy_logits, batch_values = model(batch_state)
                            
                            # ë°°ì¹˜ ì‘ë‹µì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë¶„í•´í•´ì„œ ì „ì†¡
                            policies_list = [batch_policy_logits[i].cpu() for i in range(len(batch_tensors))]
                            values_list = [batch_values[i].cpu() for i in range(len(batch_tensors))]
                            
                            response_queue.put((worker_id, request_id, policies_list, values_list))
                    
                    batch_count += 1
                    total_requests += len(batch_requests)
                    
                    # ì‹¤ì œ í…ì„œ ê°œìˆ˜ ê³„ì‚°
                    actual_tensor_count = 0
                    individual_count = 0
                    batch_request_count = 0
                    
                    for _, _, state in batch_requests:
                        if isinstance(state, list):
                            batch_request_count += len(state)
                            actual_tensor_count += len(state)
                        else:
                            individual_count += 1
                            actual_tensor_count += 1
                    
                    # ë” ìƒì„¸í•œ í†µê³„ ì¶œë ¥
                    if batch_count % 5 == 0:  # 5ë°°ì¹˜ë§ˆë‹¤ ì¶œë ¥
                        elapsed_time = time.time() - start_time
                        batches_per_sec = batch_count / elapsed_time if elapsed_time > 0 else 0
                        avg_batch_size = total_requests / batch_count if batch_count > 0 else 0
                        avg_tensor_count = actual_tensor_count / len(batch_requests) if len(batch_requests) > 0 else 0
                        
                        print(f"ğŸ“Š Batch {batch_count}: {total_requests} msgs, {actual_tensor_count} tensors, avg msg/batch: {avg_batch_size:.1f}, avg tensor/msg: {avg_tensor_count:.1f}, {batches_per_sec:.1f} batches/sec")
                    
                    # ë°°ì¹˜ í¬ê¸°ë³„ í†µê³„
                    if actual_tensor_count >= 500:  # í° ë°°ì¹˜ ê°ì§€
                        print(f"ğŸš€ Large batch processed: {actual_tensor_count} tensors ({len(batch_requests)} messages) in batch {batch_count}")
                    
                    # ë°°ì¹˜ ìš”ì²­ vs ê°œë³„ ìš”ì²­ ë¹„ìœ¨
                    if batch_count % 20 == 0:  # 20ë°°ì¹˜ë§ˆë‹¤ ìƒì„¸ í†µê³„
                        print(f"ğŸ“ˆ Batch {batch_count}: Individual tensors: {individual_count}, Batch tensors: {batch_request_count}, Total: {actual_tensor_count}")
                    
                    batch_requests = []
                    last_process_time = time.time()
            else:
                time.sleep(0.0001)
                
        except Exception as e:
            print(f"âŒ Server error: {e}")
            break
    
    print(f"ğŸ”¥ Stable inference server stopped. Total: {batch_count} batches, {total_requests} requests")

def stable_selfplay_worker(worker_id, game_queue, request_queue, response_queue, gui_queue=None):
    """ì•ˆì •ì ì¸ ìê¸°ëŒ€êµ­ ì›Œì»¤"""
    print(f"ğŸš€ Stable Worker {worker_id} starting")
    
    # numba ì›Œë°ì—…
    try:
        from .warmup import warmup_numba_functions
        warmup_numba_functions()
    except Exception as e:
        print(f"âš ï¸ Worker {worker_id} warmup failed: {e}")
    
    from .game_player import play_game
    from ..core.position import Position, parse_fen
    from ..core.constants import start_position
    from .utils import position_to_tensor
    from .config import MCTS_SIMULATIONS
    
    games_played = 0
    total_time = 0
    
    try:
        while True:
            try:
                game_id = game_queue.get_nowait()
            except queue.Empty:
                break
            
            start_time = time.time()
            
            # ì…€í”„í”Œë ˆì´ ê²Œì„ ì„¤ì •
            game_config = {
                'mode': 'selfplay',
                'device': torch.device("cuda" if torch.cuda.is_available() else "cpu"),
                'temperature': 1.0,
                'dirichlet_noise': True,
                'mcts_simulations': MCTS_SIMULATIONS,
                'request_q': request_queue,
                'response_q': response_queue,
                'max_moves': 200,
                'save_history': True
            }
            
            # ë‹¨ì¼ ëª¨ë¸ë¡œ ì…€í”„í”Œë ˆì´ (ëª¨ë¸ì€ inference ì„œë²„ì—ì„œ ì²˜ë¦¬)
            models = {'white': None, 'black': None}  # inference ì„œë²„ ì‚¬ìš©
            
            # ì‹¤ì œ ê²Œì„ í”Œë ˆì´
            result = play_game(worker_id, game_id, models, game_config, gui_queue)
            
            elapsed = time.time() - start_time
            total_time += elapsed
            games_played += 1
            
            if games_played % 10 == 0:
                avg_time = total_time / games_played
                print(f"ğŸ® Worker {worker_id}: {games_played} games, avg {avg_time*1000:.1f}ms/game")
            
            # GUI ì—…ë°ì´íŠ¸
            if gui_queue and games_played % 5 == 0:
                try:
                    gui_queue.put({'type': 'game_completed', 'worker_id': worker_id, 'game_id': game_id})
                except:
                    pass
                    
    except Exception as e:
        print(f"âŒ Worker {worker_id} error: {e}")
    
    print(f"ğŸ¯ Stable Worker {worker_id} completed {games_played} games")
